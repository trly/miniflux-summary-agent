"""Pydantic models for Miniflux API responses and AI processing."""

from datetime import datetime
from typing import Optional
from enum import Enum
from pydantic import BaseModel, Field, AnyHttpUrl, ConfigDict
import re
import requests
from bs4 import BeautifulSoup
import logging


class MinifluxBase(BaseModel):
    """Base class for Miniflux API models with forgiving validation."""
    model_config = ConfigDict(extra="ignore", populate_by_name=True)


# Note: CategoryEnum removed - categories now come from Miniflux feed categories


class Category(MinifluxBase):
    """Miniflux feed category."""
    id: int = Field(..., description="Category ID")
    user_id: int = Field(..., description="User ID who owns this category")
    title: Optional[str] = Field(None, description="Category title")


class Icon(MinifluxBase):
    """Miniflux feed icon."""
    feed_id: int = Field(..., description="Feed ID this icon belongs to")
    icon_id: int = Field(..., description="Icon ID")


class Feed(MinifluxBase):
    """Miniflux feed information."""
    id: int = Field(..., description="Feed ID")
    user_id: int = Field(..., description="User ID who owns this feed")
    title: str = Field(..., description="Feed title")
    site_url: Optional[str] = Field(None, description="Website URL")
    feed_url: Optional[str] = Field(None, description="Feed URL")
    checked_at: Optional[str] = Field(None, description="Last time feed was checked")
    etag_header: Optional[str] = Field(None, description="ETag header from last request")
    last_modified_header: Optional[str] = Field(None, description="Last-Modified header from last request")
    parsing_error_message: Optional[str] = Field(None, description="Error message if parsing failed")
    parsing_error_count: Optional[int] = Field(None, description="Number of parsing errors")
    scraper_rules: Optional[str] = Field(None, description="Scraper rules")
    rewrite_rules: Optional[str] = Field(None, description="Rewrite rules")
    crawler: Optional[bool] = Field(None, description="Whether crawler is enabled")
    blocklist_rules: Optional[str] = Field(None, description="Blocklist rules")
    keeplist_rules: Optional[str] = Field(None, description="Keeplist rules")
    user_agent: Optional[str] = Field(None, description="User agent string")
    username: Optional[str] = Field(None, description="Authentication username")
    password: Optional[str] = Field(None, description="Authentication password")
    disabled: Optional[bool] = Field(None, description="Whether feed is disabled")
    ignore_http_cache: Optional[bool] = Field(None, description="Whether to ignore HTTP cache")
    fetch_via_proxy: Optional[bool] = Field(None, description="Whether to fetch via proxy")
    category: Optional[Category] = Field(None, description="Feed category")
    icon: Optional[Icon] = Field(None, description="Feed icon")


class Entry(MinifluxBase):
    """Miniflux entry (article)."""
    id: int = Field(..., description="Entry ID")
    user_id: int = Field(..., description="User ID who owns this entry")
    feed_id: int = Field(..., description="Feed ID this entry belongs to")
    title: Optional[str] = Field(None, description="Entry title")
    url: Optional[str] = Field(None, description="Entry URL")
    comments_url: Optional[str] = Field(None, description="Comments URL")
    author: Optional[str] = Field(None, description="Entry author")
    content: Optional[str] = Field(None, description="Entry content (HTML)")
    hash: Optional[str] = Field(None, description="Entry hash")
    published_at: Optional[str] = Field(None, description="When entry was published")
    created_at: Optional[str] = Field(None, description="When entry was created in Miniflux")
    status: Optional[str] = Field(None, description="Entry status (read/unread)")
    share_code: Optional[str] = Field(None, description="Share code")
    starred: Optional[bool] = Field(None, description="Whether entry is starred")
    reading_time: Optional[int] = Field(None, description="Estimated reading time in minutes")
    enclosures: Optional[list] = Field(None, description="Entry enclosures")
    feed: Optional[Feed] = Field(None, description="Feed information")


class EntriesResponse(MinifluxBase):
    """Response from Miniflux get_entries API."""
    total: int = Field(..., description="Total number of entries")
    entries: list[Entry] = Field(..., description="List of entries")


def strip_html(html_content: str) -> str:
    """Strip HTML tags and clean up text content."""
    if not html_content:
        return ""
    
    # Remove HTML tags, replacing them with spaces only if needed
    clean_text = re.sub(r'<[^>]+>', ' ', html_content)
    
    # Clean up whitespace (multiple spaces become single space)
    clean_text = re.sub(r'\s+', ' ', clean_text).strip()
    
    # Fix spaces before punctuation
    clean_text = re.sub(r'\s+([.!?,:;])', r'\1', clean_text)
    
    return clean_text


def truncate_content(content: str, max_length: int) -> tuple[str, bool]:
    """Truncate content if needed and return (content, was_truncated)."""
    if len(content) <= max_length:
        return content, False
    
    # Truncate and add ellipsis
    truncated = content[:max_length - 1] + "â€¦"
    return truncated, True


def is_single_sentence_summary(content: str) -> bool:
    """
    Detect if content appears to be just a single sentence summary.
    
    Args:
        content: The content to analyze
        
    Returns:
        bool: True if content appears to be a single sentence summary
    """
    if not content or len(content.strip()) < 20:
        return True
    
    # Clean content for analysis
    clean_content = strip_html(content).strip()
    
    # Count sentences (look for sentence endings)
    sentence_endings = re.findall(r'[.!?]+(?:\s|$)', clean_content)
    sentence_count = len(sentence_endings)
    
    # Consider it a summary if:
    # 1. Very short content (less than 100 chars)
    # 2. Only 1-2 sentences and less than 300 chars
    # 3. Contains summary-like keywords and less than 400 chars
    summary_keywords = ['summary', 'excerpt', 'brief', 'overview', 'abstract']
    has_summary_keywords = any(keyword in clean_content.lower() for keyword in summary_keywords)
    
    return (
        len(clean_content) < 100 or
        (sentence_count <= 2 and len(clean_content) < 300) or
        (has_summary_keywords and len(clean_content) < 400)
    )


async def fetch_article_content(url: str, timeout: int = 10) -> Optional[str]:
    """
    Fetch the full article content from a URL.
    
    Args:
        url: The article URL to fetch
        timeout: Request timeout in seconds
        
    Returns:
        str: The article content, or None if fetching failed
    """
    logger = logging.getLogger(__name__)
    
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        response = requests.get(url, headers=headers, timeout=timeout)
        response.raise_for_status()
        
        # Parse HTML content
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # Remove script and style elements
        for script in soup(["script", "style", "nav", "header", "footer"]):
            script.decompose()
        
        # Try to find main content area
        content_selectors = [
            'article', '[role="main"]', '.content', '.post-content', 
            '.article-content', '.entry-content', 'main', '.main-content'
        ]
        
        content_element = None
        for selector in content_selectors:
            content_element = soup.select_one(selector)
            if content_element:
                break
        
        # Fallback to body if no specific content area found
        if not content_element:
            content_element = soup.find('body')
        
        if content_element:
            # Extract text content
            text_content = content_element.get_text(separator=' ', strip=True)
            
            # Clean up the text
            text_content = re.sub(r'\s+', ' ', text_content).strip()
            
            # Return content if it's substantial
            if len(text_content) > 200:
                return text_content
        
        logger.warning(f"Could not extract substantial content from {url}")
        return None
        
    except requests.RequestException as e:
        logger.warning(f"Failed to fetch article content from {url}: {e}")
        return None
    except Exception as e:
        logger.error(f"Unexpected error fetching article content from {url}: {e}")
        return None


class ArticleInput(BaseModel):
    """Simplified article input for AI processing."""
    id: int = Field(..., description="Article ID")
    title: str = Field(..., description="Article title")
    url: str = Field(..., description="Article URL")
    published_at: Optional[str] = Field(None, description="When article was published")
    content: str = Field(..., description="Article content (plain text, truncated if needed)")
    source: str = Field(..., description="Source feed title")
    author: str = Field(..., description="Article author")
    category: str = Field(..., description="Feed category from Miniflux")
    truncated: bool = Field(False, description="Whether content was truncated")
    
    @classmethod
    async def from_entry(cls, entry: Entry, max_content_length: int = 500, fetch_full_content: bool = True) -> "ArticleInput":
        """Create ArticleInput from Miniflux Entry."""
        # Strip HTML and get plain text
        plain_content = strip_html(entry.content or "")
        
        # Check if we should fetch full content
        should_fetch = (
            fetch_full_content and 
            entry.url and 
            is_single_sentence_summary(plain_content)
        )
        
        if should_fetch:
            logger = logging.getLogger(__name__)
            logger.debug(f"Detected summary content for article {entry.id}, fetching full content from {entry.url}")
            
            # Try to fetch full article content
            full_content = await fetch_article_content(entry.url)
            if full_content:
                plain_content = full_content
                logger.debug(f"Successfully fetched {len(full_content)} chars for article {entry.id}")
            else:
                logger.warning(f"Failed to fetch full content for article {entry.id}, using original content")
        
        # Truncate if needed with ellipsis
        content, truncated = truncate_content(plain_content, max_content_length)
        
        # Get category from feed
        category = "Uncategorized"
        if entry.feed and entry.feed.category and entry.feed.category.title:
            category = entry.feed.category.title
        
        return cls(
            id=entry.id,
            title=entry.title or "Untitled",
            url=entry.url or "",
            published_at=entry.published_at,
            content=content,
            source=entry.feed.title if entry.feed else "Unknown",
            author=entry.author or "Unknown",
            category=category,
            truncated=truncated
        )


class ArticleSummary(BaseModel):
    """AI-generated summary of an article."""
    id: int = Field(..., description="Original article ID")
    title: str = Field(..., description="Article title")
    summary: str = Field(..., description="2-4 sentence summary")
    category: str = Field(..., description="Feed category from Miniflux")
    source: str = Field(..., description="Source feed title")
    url: str = Field(..., description="Article URL")
    published_at: Optional[str] = Field(None, description="When article was published")
    author: str = Field(..., description="Article author")
    truncated: bool = Field(False, description="Whether content was truncated")


# NOTE: These models are unused but kept for potential future use
# class CategorySummary(BaseModel):
#     """Summary of articles in a category."""
#     category: CategoryEnum = Field(..., description="Category name")
#     count: int = Field(..., description="Number of articles in this category")
#     articles: list[ArticleSummary] = Field(..., description="Articles in this category")


# class SummaryReport(BaseModel):
#     """Final summary report."""
#     total_articles: int = Field(..., description="Total number of articles processed")
#     categories: list[CategorySummary] = Field(..., description="Articles grouped by category")
#     generated_at: datetime = Field(..., description="When this report was generated")
